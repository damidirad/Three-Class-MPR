{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d801039",
   "metadata": {},
   "source": [
    "# Preprocessing the LastFM-360K Dataset\n",
    "\n",
    "This notebook outlines the preprocessing steps used to create a synthetic [LastFM-360K dataset (2010)](http://ocelma.net/MusicRecommendationDataset/lastfm-360K.html/) with three gender classes (m/f/nb) as the sensitive attribute. A single-step filter is applied to the interaction data to remove sparse users/items and keep a dense, well-supported subset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb0dbfb",
   "metadata": {},
   "source": [
    "## Imports and Configuration\n",
    "\n",
    "The configuration reassigns 10% of users to the non-binary class. The interaction data is filtered to retain users with at least 50 unique attributes and attributes with at least 20 unique users. Ratings are binarized using a 2.5 threshold on the 1–5 scale.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba683c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "DATA_DIR = Path(os.getenv('PROJECT_ROOT', Path.cwd()))\n",
    "\n",
    "NON_BINARY_FRAC = 0.1\n",
    "RANDOM_SEED = 42\n",
    "MIN_ARTISTS_PER_USER = 50\n",
    "MIN_USERS_PER_ARTIST = 20\n",
    "THRESHOLD_LABEL = 2.5\n",
    "\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2add851",
   "metadata": {},
   "source": [
    "## Load User Data\n",
    "\n",
    "User data is loaded and reduced to user IDs and gender labels, which serve as the sensitive attribute; rows with missing values are dropped and all other columns are discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44fa7782",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_csv(\n",
    "    filepath_or_buffer='usersha1-profile.tsv',\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    usecols=[0, 1],\n",
    "    names=['user_id', 'gender']\n",
    ").dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bb0a75",
   "metadata": {},
   "source": [
    "## Create Non-Binary Gender Class\n",
    "\n",
    "A synthetic non-binary attribute is created by randomly sampling 10% of users from the existing male and female populations while preserving their original ratio. Gender labels are displayed before and after the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5345b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INITIAL GENDER DISTRIBUTION\n",
      "============================================================\n",
      "Male:   241642 (74.0%)\n",
      "Female:  84930 (26.0%) \n",
      "\n",
      "============================================================\n",
      "ASSIGNING NON-BINARY GENDERS\n",
      "============================================================\n",
      "Sampling 10% of users to be non-binary.\n",
      "Sampling respects the existing M/F ratio (2.845):\n",
      "  - 24165 from male users\n",
      "  - 8492 from female users \n",
      "\n",
      "============================================================\n",
      "RESULTING GENDER DISTRIBUTION\n",
      "============================================================\n",
      "Men:        217477 (66.6%)\n",
      "Women:       76438 (23.4%)\n",
      "Non-binary:  32657 (10.0%) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "male_count = users_df[users_df['gender'] == 'm'].shape[0]\n",
    "female_count = users_df[users_df['gender'] == 'f'].shape[0]\n",
    "total_users = male_count + female_count\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INITIAL GENDER DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Male:':<7} {male_count:<3} ({male_count/total_users*100:>4.1f}%)\")\n",
    "print(f\"{'Female:':<8} {female_count:<3} ({female_count/total_users*100:>4.1f}%) \\n\")\n",
    "\n",
    "num_non_binary = int(total_users * NON_BINARY_FRAC)\n",
    "\n",
    "# Sample users to become non-binary (respecting existing gender ratio)\n",
    "gender_counts = users_df['gender'].value_counts()\n",
    "ratio_m_f = gender_counts['m'] / gender_counts['f']\n",
    "num_nb_from_female = int(num_non_binary / (1 + ratio_m_f))\n",
    "num_nb_from_male = num_non_binary - num_nb_from_female\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ASSIGNING NON-BINARY GENDERS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Sampling {NON_BINARY_FRAC*100:.0f}% of users to be non-binary.\")\n",
    "print(f\"Sampling respects the existing M/F ratio ({ratio_m_f:.3f}):\")\n",
    "print(f\"  - {num_nb_from_male} from male users\")\n",
    "print(f\"  - {num_nb_from_female} from female users \\n\")\n",
    "\n",
    "male_indices = users_df[users_df['gender'] == 'm'].sample(\n",
    "    n=num_nb_from_male, random_state=RANDOM_SEED\n",
    ").index\n",
    "female_indices = users_df[users_df['gender'] == 'f'].sample(\n",
    "    n=num_nb_from_female, random_state=RANDOM_SEED\n",
    ").index\n",
    "\n",
    "# Combine and assign non-binary\n",
    "nb_indices = male_indices.union(female_indices)\n",
    "users_df.loc[nb_indices, 'gender'] = 'nb'\n",
    "\n",
    "male_count = users_df[users_df['gender'] == 'm'].shape[0]\n",
    "female_count = users_df[users_df['gender'] == 'f'].shape[0]\n",
    "nb_count = users_df[users_df['gender'] == 'nb'].shape[0]\n",
    "\n",
    "assert total_users == male_count + female_count + nb_count, (\n",
    "    f\"Population mismatch after assigning non-binary genders. \"\n",
    "    f\"Before: {total_users}, \"\n",
    "    f\"After: {male_count + female_count + nb_count}\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RESULTING GENDER DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Men:':<11} {male_count:<3} ({male_count/total_users*100:>4.1f}%)\")\n",
    "print(f\"{'Women:':<12} {female_count:<3} ({female_count/total_users*100:>4.1f}%)\")\n",
    "print(f\"{'Non-binary:':<12} {nb_count:<3} ({nb_count/total_users*100:>4.1f}%) \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b68af4d",
   "metadata": {},
   "source": [
    "## Load Interaction Data\n",
    "\n",
    "User–item interactions are loaded by retaining only the relevant columns. Rows with missing values are dropped, and interactions are restricted to users present in the user data to ensure consistent user IDs across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e05970a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = pd.read_csv(\n",
    "    filepath_or_buffer='usersha1-artmbid-artname-plays.tsv',\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    usecols=[0, 2, 3], \n",
    "    names=['user_id', 'item_id', 'plays']\n",
    ").dropna()\n",
    "\n",
    "valid_users = set(users_df['user_id'].unique())\n",
    "items_df = items_df[items_df['user_id'].isin(valid_users)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118292c7",
   "metadata": {},
   "source": [
    "## Interaction Filtering\n",
    "\n",
    "The interaction data is filtered to retain a dense and well-supported subset. Users with fewer than 50 unique artists are removed, after which artists listened to by fewer than 20 users are discarded. Summary statistics are reported before and after filtering to verify the effect of these constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "583e6732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BEFORE FILTERING\n",
      "============================================================\n",
      "Total interactions: 15,947,926\n",
      "Min artists per user: 1\n",
      "Min users per artist: 1\n",
      "\n",
      "============================================================\n",
      "AFTER FILTERING\n",
      "============================================================\n",
      "Total interactions: 6,792,309\n",
      "Min artists per user: 6\n",
      "Min users per artist: 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_before_filtering = set(items_df['user_id'].unique())\n",
    "artists_per_user_before = items_df.groupby('user_id')['item_id'].nunique()\n",
    "users_per_artist_before = items_df.groupby('item_id')['user_id'].nunique()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BEFORE FILTERING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total interactions: {len(items_df):,}\")\n",
    "print(f\"Min artists per user: {artists_per_user_before.min()}\")\n",
    "print(f\"Min users per artist: {users_per_artist_before.min()}\\n\")\n",
    "\n",
    "active_users = artists_per_user_before[artists_per_user_before >= MIN_ARTISTS_PER_USER].index\n",
    "items_df = items_df[items_df['user_id'].isin(active_users)]\n",
    "\n",
    "# Recalculate artist statistics after user filtering\n",
    "users_per_artist_filtered = items_df.groupby('item_id')['user_id'].nunique()\n",
    "\n",
    "popular_artists = users_per_artist_filtered[users_per_artist_filtered >= MIN_USERS_PER_ARTIST].index\n",
    "items_df = items_df[items_df['item_id'].isin(popular_artists)]\n",
    "\n",
    "users_after_filtering = set(items_df['user_id'].unique())\n",
    "artists_per_user_after = items_df.groupby('user_id')['item_id'].nunique()\n",
    "users_per_artist_after = items_df.groupby('item_id')['user_id'].nunique()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"AFTER FILTERING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total interactions: {len(items_df):,}\")\n",
    "print(f\"Min artists per user: {artists_per_user_after.min()}\")\n",
    "print(f\"Min users per artist: {users_per_artist_after.min()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1694a6e",
   "metadata": {},
   "source": [
    "## Update Users DataFrame\n",
    "\n",
    "User data is synchronized with the filtered interaction data by removing users that were eliminated during interaction filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d7dea8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194156 users removed during filtering. Updating users dataframe... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "removed_users = users_before_filtering.difference(users_after_filtering)\n",
    "\n",
    "if len(removed_users) > 0:\n",
    "    print(f\"{len(removed_users)} users removed during filtering. Updating users dataframe... \\n\")\n",
    "    valid_user_ids = items_df['user_id'].unique()\n",
    "    users_df = users_df[users_df['user_id'].isin(valid_user_ids)].reset_index(drop=True)\n",
    "elif len(removed_users) == 0:\n",
    "    print(\"No users removed during filtering. Proceeding without updating users dataframe. \\n\")\n",
    "else:\n",
    "    raise ValueError(\"Unexpected condition: more users after filtering than before. \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685806f8",
   "metadata": {},
   "source": [
    "## Label Construction\n",
    "\n",
    "Play counts are log-transformed to reduce skew and then rescaled to a 1–5 range. The rescaled values are binarized using a 2.5 threshold to create the final labels. Intermediate columns are removed and the final label distribution is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f985fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LABEL DISTRIBUTION\n",
      "============================================================\n",
      "Label 1: 2,665,018 (39.2%)\n",
      "Label 0: 4,127,291 (60.8%) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "items_df['log_plays'] = np.log1p(items_df['plays'])\n",
    "\n",
    "# Min-max normalize to [1,5]\n",
    "min_log = items_df['log_plays'].min()\n",
    "max_log = items_df['log_plays'].max()\n",
    "items_df['rating'] = 1 + (items_df['log_plays'] - min_log) * 4 / (max_log - min_log)\n",
    "\n",
    "items_df['label'] = (items_df['rating'] >= THRESHOLD_LABEL).astype(int)\n",
    "\n",
    "items_df = items_df.drop(columns=['plays', 'log_plays', 'rating'])\n",
    "\n",
    "total_labels = len(items_df)\n",
    "num_label_1 = (items_df['label'] == 1).sum()\n",
    "num_label_0 = (items_df['label'] == 0).sum()\n",
    "pct_label_1 = num_label_1 / total_labels * 100\n",
    "pct_label_0 = num_label_0 / total_labels * 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LABEL DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Label 1: {num_label_1:,} ({pct_label_1:.1f}%)\")\n",
    "print(f\"Label 0: {num_label_0:,} ({pct_label_0:.1f}%) \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a9903",
   "metadata": {},
   "source": [
    "## ID Remapping and Consistency Checks\n",
    "\n",
    "User and item identifiers are remapped to consecutive integer ranges to ensure consistent indexing and model compatibility. Musical items, originally represented by non-numeric identifiers, are encoded as integer IDs. Consistency checks are performed to verify that user and item IDs are contiguous and aligned across the interaction and user data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb7fd7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RANGE CONSISTENCY CHECK\n",
      "============================================================\n",
      "Users ID range: 0 - 131986\n",
      "Items ID range: 0 - 131986\n",
      "Users consistent with items: True \n",
      "\n",
      "Item ID range: 0 - 26678\n",
      "Item IDs are consecutive: True\n"
     ]
    }
   ],
   "source": [
    "user_ids = users_df['user_id'].unique()\n",
    "user_id_map = {old_id: new_id for new_id, old_id in enumerate(user_ids)}\n",
    "users_df['user_id'] = users_df['user_id'].map(user_id_map)\n",
    "items_df['user_id'] = items_df['user_id'].map(user_id_map)\n",
    "\n",
    "artist_ids = items_df['item_id'].unique()\n",
    "artist_id_map = {old_id: new_id for new_id, old_id in enumerate(artist_ids)}\n",
    "items_df['item_id'] = items_df['item_id'].map(artist_id_map)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RANGE CONSISTENCY CHECK\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Users ID range:\", users_df['user_id'].min(), \"-\", users_df['user_id'].max())\n",
    "print(\"Items ID range:\", items_df['user_id'].min(), \"-\", items_df['user_id'].max())\n",
    "print(\"Users consistent with items:\", items_df['user_id'].isin(users_df['user_id']).all(), \"\\n\")\n",
    "\n",
    "print(\"Item ID range:\", items_df['item_id'].min(), \"-\", items_df['item_id'].max())\n",
    "print(\"Item IDs are consecutive:\", items_df['item_id'].max() == items_df['item_id'].nunique() - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845e4412",
   "metadata": {},
   "source": [
    "## Train/Val/Test Split\n",
    "\n",
    "Interactions are split per user (80% train, 10% validation, 10% test) so that users are represented across splits. This reduces distribution shifts between splits and supports a more realistic evaluation setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c2a443c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET SPLITS\n",
      "============================================================\n",
      "Train size: 5,386,464\n",
      "Valid size:   618,118\n",
      "Test size:    787,727\n",
      "Total size: 6,792,309 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def split_per_user(df, train_frac=0.8, val_frac=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Splitting items per user by using groupby and apply.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with user_id column\n",
    "        train_frac: Fraction for training\n",
    "        val_frac: Fraction for validation\n",
    "        random_state: Random seed\n",
    "\n",
    "    Returns:\n",
    "        train_df, val_df, test_df\n",
    "    \"\"\"\n",
    "    def split_user_data(user_items):\n",
    "        \"\"\"\n",
    "        Split a single user's interactions into train/val/test sets.\n",
    "        \"\"\"\n",
    "        user_id = user_items.name\n",
    "        user_items = user_items.copy()\n",
    "        user_items['user_id'] = user_id\n",
    "\n",
    "        user_items = user_items.sample(frac=1, random_state=random_state) # Shuffle items\n",
    "        num_items = len(user_items)\n",
    "\n",
    "        num_train = int(train_frac * num_items)\n",
    "        num_val = int(val_frac * num_items)\n",
    "\n",
    "        split_labels = ['train'] * num_train + ['val'] * num_val + ['test'] * (num_items - num_train - num_val)\n",
    "        user_items['split'] = split_labels\n",
    "\n",
    "        return user_items\n",
    "\n",
    "    # Apply splitting to each user's items\n",
    "    df_with_splits = df.groupby('user_id', group_keys=False).apply(split_user_data)\n",
    "    df_with_splits = df_with_splits[['user_id', 'item_id', 'label', 'split']] # Reorder columns\n",
    "\n",
    "    train_df = df_with_splits[df_with_splits['split'] == 'train'].drop(columns=['split'])\n",
    "    valid_df = df_with_splits[df_with_splits['split'] == 'val'].drop(columns=['split'])\n",
    "    test_df = df_with_splits[df_with_splits['split'] == 'test'].drop(columns=['split'])\n",
    "\n",
    "    return (\n",
    "        train_df.reset_index(drop=True),\n",
    "        valid_df.reset_index(drop=True),\n",
    "        test_df.reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "train_df, valid_df, test_df = split_per_user(items_df, random_state=RANDOM_SEED)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET SPLITS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Train size:':<11} {len(train_df):>5,}\")\n",
    "print(f\"{'Valid size:':<13} {len(valid_df):>5,}\")\n",
    "print(f\"{'Test size:':<13} {len(test_df):>5,}\")\n",
    "print(f\"{'Total size:':<11} {len(train_df) + len(valid_df) + len(test_df):>5,} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed6e38f",
   "metadata": {},
   "source": [
    "## Summary and Save Output Files\n",
    "\n",
    "Final dataset statistics are displayed, gender labels are mapped to integers (m=0, f=1, nb=2), and all processed files are saved to CSV format, including both ordered and randomized versions of the sensitive attribute data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23f9380e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL DATASET SUMMARY\n",
      "============================================================\n",
      "Total users: 131987\n",
      "Total items: 26679\n",
      "Total interactions: 6,792,309\n",
      "\n",
      "Final gender distribution:\n",
      " - Male:       91567 (69.4%)\n",
      " - Female:     27208 (20.6%)\n",
      " - Non-binary: 13212 (10.0%) \n",
      "\n",
      "Mapping gender labels to integers...\n",
      "\n",
      "Saving processed files to: Three-Class-MPR/datasets/Lastfm-360K-synthetic \n",
      "\n",
      "✓ All files saved successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FINAL DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total users: {len(users_df)}\")\n",
    "print(f\"Total items: {items_df['item_id'].nunique()}\")\n",
    "print(f\"Total interactions: {len(train_df) + len(valid_df) + len(test_df):,}\")\n",
    "\n",
    "male_final = (users_df['gender'] == 'm').sum()\n",
    "female_final = (users_df['gender'] == 'f').sum()\n",
    "nb_final = (users_df['gender'] == 'nb').sum()\n",
    "\n",
    "print(f\"\\nFinal gender distribution:\")\n",
    "print(f\" - {'Male:':<11} {male_final:>4} ({male_final/len(users_df)*100:.1f}%)\")\n",
    "print(f\" - {'Female:':<11} {female_final:>4} ({female_final/len(users_df)*100:.1f}%)\")\n",
    "print(f\" - {'Non-binary:':<11} {nb_final:>4} ({nb_final/len(users_df)*100:.1f}%) \\n\")\n",
    "\n",
    "print(\"Mapping gender labels to integers...\")\n",
    "gender_mapping = {'m': 0, 'f': 1, 'nb': 2}\n",
    "users_df['gender'] = users_df['gender'].map(gender_mapping)\n",
    "\n",
    "# Randomized sensitive attribute dataset\n",
    "users_random = users_df.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nSaving processed files to: {DATA_DIR.parent.parent.name}/{DATA_DIR.parent.name}/{DATA_DIR.name} \\n\")\n",
    "\n",
    "users_df.to_csv(DATA_DIR / 'sensitive_attribute.csv', index=False)\n",
    "users_random.to_csv(DATA_DIR / 'sensitive_attribute_random.csv', index=False)\n",
    "train_df.to_csv(DATA_DIR / 'train.csv', index=False)\n",
    "valid_df.to_csv(DATA_DIR / 'valid.csv', index=False)\n",
    "test_df.to_csv(DATA_DIR / 'test.csv', index=False)\n",
    "\n",
    "print(\"✓ All files saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fact_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
